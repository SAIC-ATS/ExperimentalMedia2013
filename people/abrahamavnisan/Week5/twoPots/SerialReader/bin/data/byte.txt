Thebyte/ˈbaɪt/isaunitofdigitalinformationincomputingandtelecommunicationsthatmostcommonlyconsistsofeightbits.Historically,thebytewasthenumberofbitsusedtoencodeasinglecharacteroftextinacomputerandforthisreasonitisthesmallestaddressableunitofmemoryinmanycomputerarchitectures.Thesizeofthebytehashistoricallybeenhardwaredependentandnodefinitivestandardsexistedthatmandatedthesize.Thedefactostandardofeightbitsisaconvenientpoweroftwopermittingthevalues0through255foronebyte.TheinternationalstandardISO/IEC80000-13codifiedthiscommonmeaning.Manytypesofapplicationsuseinformationrepresentableineightorfewerbitsandprocessordesignersoptimizeforthiscommonusage.Thepopularityofmajorcommercialcomputingarchitectureshasaidedintheubiquitousacceptanceofthe8-bitsize.Theunitoctetwasdefinedtoexplicitlydenoteasequenceof8bitsbecauseoftheambiguityassociatedatthetimewiththebyte.HistoryThetermbytewascoinedbyWernerBuchholzinJuly1956,duringtheearlydesignphasefortheIBMStretchcomputer.Itisadeliberaterespellingofbitetoavoidaccidentalmutationtobit.Earlycomputersusedavarietyof4-bitbinarycodeddecimal(BCD)representationsandthe6-bitcodesforprintablegraphicpatternscommonintheU.S.Army(Fieldata)andNavy.Theserepresentationsincludedalphanumericcharactersandspecialgraphicalsymbols.Thesesetswereexpandedin1963to7bitsofcoding,calledtheAmericanStandardCodeforInformation Interchange